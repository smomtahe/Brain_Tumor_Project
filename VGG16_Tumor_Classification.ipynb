{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MTxeWHxxkY20"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os, io,zipfile\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import cv2\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data - data has training (tumor,no-tumor) and test (tumor,no-tumor) datasets\n",
        "\n",
        "zip_file_path=(\"/content/MRIBrainTumor.zip\")\n",
        "extracted_dir=(\"/content/extracted_data\")\n",
        "!mkdir -p $extracted_dir\n",
        "with zipfile.ZipFile(zip_file_path,'r') as zip_ref:\n",
        "  zip_ref.extractall(extracted_dir)\n",
        "!ls $extracted_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J24VGsu_lf5c",
        "outputId": "9c1075cc-7cf2-4936-d1ee-a2b28ffdaa76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing  Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# path_to_no_tumor_directory and \"path_to_pituitary_tumor_directory\"\n",
        "no_tumor_path = Path(\"/content/extracted_data/Training/no_tumor\")\n",
        "pituitary_tumor_path = Path(\"/content/extracted_data/Training/pituitary_tumor\")\n",
        "\n",
        "images = []  # List for images\n",
        "labels = []  # Each time load the image and put in image array, add labels 1 tumor / 0 no-tumor to the labels array\n",
        "\n",
        "for img_path in no_tumor_path.glob(\"*jpg\"):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Load img from disk - load img files' contents to memory\n",
        "    img_array = image.img_to_array(img)  # Convert img to list of images\n",
        "\n",
        "    images.append(img_array)  # Add img to list of images\n",
        "    labels.append(0)  # For each not-tumor: value is 0\n",
        "\n",
        "for img_path in pituitary_tumor_path.glob(\"*jpg\"):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Load img from disk - load img files' contents to memory\n",
        "    img_array = image.img_to_array(img)  # Convert img to list of images\n",
        "    images.append(img_array)  # Add img to list of images\n",
        "    labels.append(1)  # For each tumor: value is 1\n",
        "\n",
        "# create a single numpy array (not a python list) with all the images we loaded\n",
        "x_train=np.array(images)\n",
        "#convert labels to a numpy array\n",
        "y_train=np.array(labels)\n"
      ],
      "metadata": {
        "id": "BqNu-OAHojKn"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train)  # 2 classes 0 and 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf_V1nTRqcDl",
        "outputId": "0d01c1eb-73f0-480e-e67f-8bbe240d0b11"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape  #(1222, 224, 224, 3)\n",
        "#y_train.shape #(1222,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f03yQtT-ywBz",
        "outputId": "dc9782c0-1031-4812-dbc7-fe06e3c69e4d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1222, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_train).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ero_wCfzGiH",
        "outputId": "8a13f646-ed4b-46d1-f1d2-70e2143c161d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    827\n",
              "0    395\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(x_train, y_train, random_state=10, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "YUcF3jN6o9HG"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtest.min(), xtest.max(),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h65Fp3Q73JoG",
        "outputId": "b2537673-2bfb-4445-e1a4-9dc4d260724c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 255.0)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature scaling  between 0 to 1\n",
        "xtrain=xtrain.astype('float32')\n",
        "xtest=xtest.astype(\"float32\")\n",
        "xtrain/=255\n",
        "xtest/=255\n",
        "print(xtrain.min(), xtrain.max())\n",
        "print(xtest.min(), xtest.max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4EO_EtV178y",
        "outputId": "0f5a9d7c-b0c9-4d16-889f-5fdf24611f50"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 1.0\n",
            "0.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encode the labels #no-tumor 0 and tumor 1 -> convert label index to categorical encoding\n",
        "# convert class vectors to binary class metrices - array with an elemnt set to 1 and the rest 0\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "ytrain=to_categorical(ytrain, 2) # number of classes is 2\n",
        "ytest=to_categorical(ytest, 2)\n",
        "ytest\n"
      ],
      "metadata": {
        "id": "uNehXA_sXEve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb6ceb9-b666-41d8-89cd-c024487f91c3"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model Convolutional Neural Network # VGG Shorten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
        "\n",
        "# maxpool: scale down convolution laers - keep largest values # dropout usually after maxpool # extra pixels left over an edge (ima size is not exactly /3) : add padding to img 0 3xtrat 0 added to edge : same padding\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3,3),padding='same', activation='relu',input_shape=(224,224,3)))  # image 2D # each filter is capable of detecting 1 pattern - sinze of window 3x3 : use when creating tiles from each image -> org to 3x3 tiles\n",
        "model.add(Conv2D(32,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25)) # % of nn to cut (bet 25 to 50)\n",
        "\n",
        "model.add(Conv2D(64,(3,3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25)) # % of nn to cut (bet 25 to 50\n",
        "\n",
        "# transition from convolution layer to dense layer : no work with D data -> flatten layer\n",
        "model.add(Flatten())  # to output\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))  # 50 : workharder to get the las answer correct\n",
        "\n",
        "model. add(Dense(2, activation='sigmoid')) # if was mroe classes : softmax\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMUtNm0q3tkV",
        "outputId": "33e90121-e72e-47ad-8e40-5a58a4794300"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_32 (Conv2D)          (None, 224, 224, 32)      896       \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 222, 222, 32)      9248      \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 111, 111, 32)      0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 111, 111, 32)      0         \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 111, 111, 64)      18496     \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 109, 109, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 54, 54, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 54, 54, 64)        0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 186624)            0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 512)               95552000  \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95618594 (364.76 MB)\n",
            "Trainable params: 95618594 (364.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "history=model.fit(xtrain,ytrain,epochs=5,shuffle=True,validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "FQQelLGVJzwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c6f3be-e956-4e22-d9c6-3156c740b445"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "28/28 [==============================] - 7s 181ms/step - loss: 1.0431 - accuracy: 0.6735 - val_loss: 0.7629 - val_accuracy: 0.7143\n",
            "Epoch 2/5\n",
            "28/28 [==============================] - 5s 168ms/step - loss: 0.4558 - accuracy: 0.8180 - val_loss: 0.3335 - val_accuracy: 0.8265\n",
            "Epoch 3/5\n",
            "28/28 [==============================] - 5s 162ms/step - loss: 0.3089 - accuracy: 0.8817 - val_loss: 0.1270 - val_accuracy: 0.9694\n",
            "Epoch 4/5\n",
            "28/28 [==============================] - 5s 164ms/step - loss: 0.1358 - accuracy: 0.9556 - val_loss: 0.0730 - val_accuracy: 0.9694\n",
            "Epoch 5/5\n",
            "28/28 [==============================] - 5s 165ms/step - loss: 0.0779 - accuracy: 0.9761 - val_loss: 0.0807 - val_accuracy: 0.9694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size=32,epochs=1,shuffle=True\n",
        "# batch size: how many at once fit in nn during training -32-128 images\n",
        "# epochs how mant times trainig data during the process\n",
        "# validation_data=(x_test,y_test), validate our training - this is data that model will never see dring training and test the acc of the training data\n",
        "# randomize the order of the training data : shuffle True"
      ],
      "metadata": {
        "id": "x6qR2dlgaS6y"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_structure=model.to_json() # save NN -write json to text\n",
        "f=Path(\"model_structure.json\") # path object\n",
        "f.write_text(model_structure)\n",
        "\n",
        "#save weights\n",
        "model.save_weights('braintumor.weights.h5')"
      ],
      "metadata": {
        "id": "98OEvEjBan_a"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Test Model Prediction\n",
        "class_labels = [\"no_tumor\", \"tumor\"]\n",
        "\n",
        "f=Path(\"/content/model_structure.json\")\n",
        "model_structure=f.read_text()\n",
        "model=model_from_json(model_structure)\n",
        "model.load_weights(\"/content/braintumor.weights.h5\")\n",
        "\n",
        "img=image.load_img(\"/content/brain.jpg\", target_size=(224,224))  # resize the image to size the nn expect 224x224\n",
        "# 3D numpy array -> nn\n",
        "image_to_test=image.img_to_array(img)/255  # RGB scale 0-1\n"
      ],
      "metadata": {
        "id": "KhhxxsX09H84"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch of images as once- create batches even 1 img\n",
        "# batches as 4D array  1: list of images,  3:image data\n",
        "# 4D add new axis to it with numpy: np.expand_dims()\n",
        "list_of_images=np.expand_dims(image_to_test,axis=0)  #axis=0 :new axis is the 1st dimension\n",
        "results=model.predict(list_of_images)\n",
        "\n",
        "single_result=results[0] # check 1st result - 1 img we have -array with 10 element - instaed of returniong 10 values - grab the array element with highest value -probability\n",
        "most_likely_class_index=int(np.argmax(single_result))\n",
        "\n",
        "# grab liklihood value\n",
        "class_likelihood=single_result[most_likely_class_index]\n",
        "class_labels=class_labels[most_likely_class_index]\n"
      ],
      "metadata": {
        "id": "E8HyOBBWfYle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860bfef4-f5a4-4df8-e8f4-bebe3b30c07e"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG : 16 layers\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import vgg16\n",
        "\n",
        "model=vgg16.VGG16()\n",
        "img = image.load_img(\"/content/brain.jpg\", target_size=(224, 224)) # vgg image size\n",
        "\n",
        "x = image.img_to_array(img) #convert img to array to feed to nn\n",
        "import numpy as np\n",
        "x = np.expand_dims(x, axis=0) # 4D\n",
        "x=vgg16.preprocess_input(x)\n",
        "\n",
        "\n",
        "predictions=model.predict(x)  # get back a 1000 element array of floating point numbers - each element in array tell us how likely our picture contains each of the 1000 object the model was training to recognize\n",
        "# decode tell us name of the most likely matches -> give top 5 most likely matches\n",
        "predicted_classes=vgg16.decode_predictions(predictions, top=9) # get top 9 mathces\n",
        "\n",
        "for imagenetid,name, likelihood in predicted_classes[0]:\n",
        "  print(\"prediction:{}-{:2f}\".format(name,likelihood))"
      ],
      "metadata": {
        "id": "3iR1ceSwfXcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8155cb6-f6f0-446e-c955-9a3122a9a259"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 24s 0us/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "prediction:pitcher-0.639047\n",
            "prediction:bolo_tie-0.058239\n",
            "prediction:vase-0.034219\n",
            "prediction:water_jug-0.034005\n",
            "prediction:buckle-0.030014\n",
            "prediction:whiskey_jug-0.029362\n",
            "prediction:mask-0.023460\n",
            "prediction:hook-0.023370\n",
            "prediction:goblet-0.014760\n"
          ]
        }
      ]
    }
  ]
}